{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f23b99b",
   "metadata": {},
   "source": [
    "### 0. Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69cc4acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991dead",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30bf6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j8/gwyf8shd0zsbnc417h9b8p9w0000gn/T/ipykernel_35118/1876213278.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  train = pd.read_csv('data/adult/adult.data', sep=',\\s', names=names, header=None)\n",
      "/var/folders/j8/gwyf8shd0zsbnc417h9b8p9w0000gn/T/ipykernel_35118/1876213278.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  test = pd.read_csv('data/adult/adult.test', sep=',\\s', names=names, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_train.shape (32561, 15) \n",
      " original_test.shape (16281, 15) \n",
      " original_data.shape (48842, 15)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "names = ['age','workclass','fnlwgt','education','education-num','marital-status',\n",
    "         'occupation','relationship', 'race','sex','capital-gain','capital-loss',\n",
    "         'hours-per-week','native-country','income']\n",
    "\n",
    "train = pd.read_csv('data/adult/adult.data', sep=',\\s', names=names, header=None)\n",
    "test = pd.read_csv('data/adult/adult.test', sep=',\\s', names=names, header=None)\n",
    "train_len = train.shape[0]\n",
    "data = pd.concat([train, test], axis=0)\n",
    "\n",
    "print('original_train.shape', train.shape,'\\n',\n",
    "      'original_test.shape', test.shape,'\\n',\n",
    "      'original_data.shape', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d7cef",
   "metadata": {},
   "source": [
    "### 2. Preprocessing and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccfc1392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# drop null values\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "# drop duplicated rows\n",
    "data = data.drop_duplicates()\n",
    "# replace \"<=50k\" and \">50k\" with 0 and 1\n",
    "data.replace(to_replace=['<=50K', '<=50K.', '>50K', '>50K.'], \n",
    "             value=[0, 0, 1, 1], inplace=True)\n",
    "# encode categorical features\n",
    "categorical_df = data.select_dtypes(object)\n",
    "numerical_df = data.select_dtypes(int, float)\n",
    "lab_enc = LabelEncoder()\n",
    "\n",
    "for column in categorical_df:\n",
    "    categorical_df[column] = lab_enc.fit_transform(categorical_df[column])\n",
    "\n",
    "data = pd.concat([categorical_df, numerical_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8bfb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7508676023463653\n",
      "0.755085886171139\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: data imbalanced\n",
    "\n",
    "train, test = data[:train_len], data[train_len:]\n",
    "\n",
    "print(sum(train['income'] == 0) / len(train))\n",
    "print(sum(test['income'] == 0) / len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e20832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "sample 1 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 2 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 3 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 4 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 5 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 6 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 7 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 8 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 9 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 10 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 11 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 12 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 13 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 14 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 15 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 16 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 17 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 18 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 19 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n",
      " \n",
      "sample 20 ...\n",
      "X_trn.shape (1000, 14) y_trn.shape (1000,) X_val.shape (500, 14) y_val.shape (500,)\n",
      "X_ori (1000, 14) y_ori (1000,) X_aug (600, 14) y_aug (600,) X_ori_aug (1600, 14) y_ori_aug (1600,)\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "\n",
    "train, test = data[:train_len], data[train_len:]\n",
    "n_samples = 20\n",
    "ratio = 0.80\n",
    "\n",
    "n_trn_per_sample = 1000\n",
    "n_trn0 = int(n_trn_per_sample * ratio)\n",
    "n_trn1 = n_trn_per_sample - n_trn0\n",
    "\n",
    "n_tst_per_sample = 500\n",
    "n_tst0 = int(n_tst_per_sample * ratio)\n",
    "n_tst1 = n_tst_per_sample - n_tst0\n",
    "\n",
    "num_neighbors = 3\n",
    "\n",
    "for i in range(n_samples):\n",
    "    \n",
    "    print(' ')\n",
    "    print('sample', i+1, '...')\n",
    "    \n",
    "    train0 = train[train['income'] == 0]\n",
    "    train1 = train[train['income'] == 1]\n",
    "    \n",
    "    test0 = test[test['income'] == 0]\n",
    "    test1 = test[test['income'] == 1]\n",
    "    \n",
    "    train0_sample = train0.sample(n=n_trn0, replace=False, random_state=i)\n",
    "    train1_sample = train1.sample(n=n_trn1, replace=False, random_state=i)\n",
    "    train_sample = pd.concat([train0_sample, train1_sample])\n",
    "    \n",
    "    \n",
    "    test0_sample = test0.sample(n=n_tst0, replace=False, random_state=i)\n",
    "    test1_sample = test1.sample(n=n_tst1, replace=False, random_state=i)\n",
    "    test_sample = pd.concat([test0_sample, test1_sample])\n",
    "    \n",
    "    X_trn, y_trn = train_sample[train_sample.columns.drop('income')], train_sample['income']\n",
    "    scaler = MinMaxScaler()\n",
    "    X_trn = scaler.fit_transform(X_trn)\n",
    "    y_trn = np.asarray(y_trn)\n",
    "\n",
    "    X_val, y_val = test_sample[test_sample.columns.drop('income')], test_sample['income']\n",
    "    scaler = MinMaxScaler()\n",
    "    X_val = scaler.fit_transform(X_val)\n",
    "    y_val = np.asarray(y_val)\n",
    "\n",
    "    print('X_trn.shape', X_trn.shape, 'y_trn.shape', y_trn.shape, \n",
    "          'X_val.shape', X_val.shape, 'y_val.shape', y_val.shape)\n",
    "    \n",
    "    # Data augmentation using BorderlineSMOTE\n",
    "    sm = BorderlineSMOTE(random_state=i, k_neighbors=num_neighbors)\n",
    "    X_res, y_res = sm.fit_resample(X_trn, y_trn)\n",
    "    X_ori = X_res[:len(X_trn)]\n",
    "    y_ori = y_res[:len(y_trn)]\n",
    "    X_aug = X_res[len(X_trn):]\n",
    "    y_aug = y_res[len(y_trn):]\n",
    "    X_ori_aug = X_res\n",
    "    y_ori_aug = y_res\n",
    "    \n",
    "    print('X_ori', X_ori.shape, 'y_ori', y_ori.shape, \n",
    "          'X_aug', X_aug.shape, 'y_aug', y_aug.shape,\n",
    "          'X_ori_aug', X_ori_aug.shape, 'y_ori_aug', y_ori_aug.shape)\n",
    "    \n",
    "    mydict = {'X_ori': X_ori, 'X_aug': X_aug, 'X_ori_aug': X_ori_aug, 'X_val': X_val,\n",
    "              'y_ori': y_ori, 'y_aug': y_aug, 'y_ori_aug': y_ori_aug, 'y_val': y_val}\n",
    "    \n",
    "    with open(\"data/adult_imbalanced_sample/augmented_sample{}_ratio={}_num_trn_per_sample={}_num_tst_per_sample={}.pickle\".format(i+1, ratio, n_trn_per_sample, n_tst_per_sample), \"wb\") as fp:\n",
    "        pickle.dump(mydict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ca0ce",
   "metadata": {},
   "source": [
    "### 3. Test the performance of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad5122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Sample 1\n",
      "AUC score for original dataset: 0.6375\n",
      "AUC score for original and augmented dataset: 0.7125\n",
      " \n",
      "Sample 2\n",
      "AUC score for original dataset: 0.56\n",
      "AUC score for original and augmented dataset: 0.7012\n",
      " \n",
      "Sample 3\n",
      "AUC score for original dataset: 0.6325\n",
      "AUC score for original and augmented dataset: 0.7337\n",
      " \n",
      "Sample 4\n",
      "AUC score for original dataset: 0.59\n",
      "AUC score for original and augmented dataset: 0.7312\n",
      " \n",
      "Sample 5\n",
      "AUC score for original dataset: 0.655\n",
      "AUC score for original and augmented dataset: 0.745\n",
      " \n",
      "Sample 6\n",
      "AUC score for original dataset: 0.6525\n",
      "AUC score for original and augmented dataset: 0.755\n",
      " \n",
      "Sample 7\n",
      "AUC score for original dataset: 0.6575\n",
      "AUC score for original and augmented dataset: 0.7813\n",
      " \n",
      "Sample 8\n",
      "AUC score for original dataset: 0.6175\n",
      "AUC score for original and augmented dataset: 0.6838\n",
      " \n",
      "Sample 9\n",
      "AUC score for original dataset: 0.5913\n",
      "AUC score for original and augmented dataset: 0.7388\n",
      " \n",
      "Sample 10\n",
      "AUC score for original dataset: 0.6\n",
      "AUC score for original and augmented dataset: 0.7475\n",
      " \n",
      "Sample 11\n",
      "AUC score for original dataset: 0.6275\n",
      "AUC score for original and augmented dataset: 0.7612\n",
      " \n",
      "Sample 12\n",
      "AUC score for original dataset: 0.5775\n",
      "AUC score for original and augmented dataset: 0.7312\n",
      " \n",
      "Sample 13\n",
      "AUC score for original dataset: 0.6875\n",
      "AUC score for original and augmented dataset: 0.7275\n",
      " \n",
      "Sample 14\n",
      "AUC score for original dataset: 0.6163\n",
      "AUC score for original and augmented dataset: 0.7688\n",
      " \n",
      "Sample 15\n",
      "AUC score for original dataset: 0.6463\n",
      "AUC score for original and augmented dataset: 0.7938\n",
      " \n",
      "Sample 16\n",
      "AUC score for original dataset: 0.6038\n",
      "AUC score for original and augmented dataset: 0.7288\n",
      " \n",
      "Sample 17\n",
      "AUC score for original dataset: 0.59\n",
      "AUC score for original and augmented dataset: 0.705\n",
      " \n",
      "Sample 18\n",
      "AUC score for original dataset: 0.5775\n",
      "AUC score for original and augmented dataset: 0.7025\n",
      " \n",
      "Sample 19\n",
      "AUC score for original dataset: 0.5612\n",
      "AUC score for original and augmented dataset: 0.725\n",
      " \n",
      "Sample 20\n",
      "AUC score for original dataset: 0.5275\n",
      "AUC score for original and augmented dataset: 0.7075\n"
     ]
    }
   ],
   "source": [
    "# Performance of the logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for i in range(n_samples):\n",
    "    \n",
    "    with open(\"data/adult_imbalanced_sample/augmented_sample{}_ratio={}_num_trn_per_sample={}_num_tst_per_sample={}.pickle\".format(i+1, ratio, n_trn_per_sample, n_tst_per_sample), \"rb\") as fp:\n",
    "        mydict = pickle.load(fp)\n",
    "    \n",
    "    X_ori = mydict['X_ori']\n",
    "    y_ori = mydict['y_ori']\n",
    "    X_aug = mydict['X_aug']\n",
    "    y_aug = mydict['y_aug']\n",
    "    X_ori_aug = mydict['X_ori_aug']\n",
    "    y_ori_aug = mydict['y_ori_aug']\n",
    "    X_val = mydict['X_val']\n",
    "    y_val = mydict['y_val']\n",
    "    \n",
    "    print(' ')\n",
    "    print('Sample', i+1)\n",
    "    \n",
    "    lr = LogisticRegression(solver='liblinear', max_iter=5000, random_state=666)\n",
    "    lr.fit(X_ori, y_ori)\n",
    "    preds = lr.predict(X_val)\n",
    "    print('AUC score for original dataset:', round(roc_auc_score(y_val, preds), 4))\n",
    "\n",
    "    lr = LogisticRegression(solver='liblinear', max_iter=5000, random_state=666)\n",
    "    lr.fit(X_ori_aug, y_ori_aug)\n",
    "    preds = lr.predict(X_val)\n",
    "    print('AUC score for original and augmented dataset:', round(roc_auc_score(y_val, preds), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a53598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
